{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the code in this notebook to train the machine learning model on Azure Machine Learning Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Don't change the code in here'''\n",
    "\n",
    "# import library \n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline \n",
    "\n",
    "# import machine learning library \n",
    "from azureml.core import Experiment, Run, Workspace\n",
    "import azureml.core\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Fill in details in the cell below</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Azure database\n",
    "import pyodbc\n",
    "server = ''\n",
    "database = ''\n",
    "username = ''\n",
    "password = ''\n",
    "driver= '{ODBC Driver 17 for SQL Server}' #don't change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Fill in details in the cell below as instructed</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to Azure Machine Learning Workspace\n",
    "subscription_id = ''\n",
    "resource_group  = '' # name of resource_group for ML workspace\n",
    "workspace_name  = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Don't change the code in here'''\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export database (description, date, quantity) to csv \n",
    "### Store it in the same directory (default location) as this notebook \n",
    "### Run the code below to process the raw data\n",
    "### A file named 'date_description_quantity.csv' should be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Don't change the code in here'''\n",
    "\n",
    "# export to csv \n",
    "import sys\n",
    "import csv\n",
    "\n",
    "export_query='SELECT order_date, order_description_and_pack_size, order_quantity FROM combined_date_formated_dataframe;'\n",
    "cnxn = pyodbc.connect('DRIVER='+driver+';SERVER='+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = cnxn.cursor()\n",
    "cursor.execute(export_query)\n",
    "result=cursor.fetchall()\n",
    "column_title = ('date', 'description', 'quantity')\n",
    "\n",
    "c = csv.writer(open('date_description_quantity.csv', 'w', newline=''))\n",
    "c.writerow(column_title)\n",
    "for x in result:\n",
    "    c.writerow(x)\n",
    "\n",
    "# close the connection and remove the cursor\n",
    "cursor.close()\n",
    "cnxn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Don't change the code in here'''\n",
    "\n",
    "# define the data preprocess function\n",
    "def preprocess(df, min_purchase, time_period, end_date):\n",
    "    from dateutil import parser\n",
    "\n",
    "    # group by date and description\n",
    "    df = df.groupby(['description','date'], as_index=False).sum()\n",
    "    df.sort_values(by = ['description'])\n",
    "\n",
    "    # count the number of times an item is perchased\n",
    "    df_2 = df.groupby('description', as_index = False).count()\n",
    "    df_2.drop(df_2.loc[df_2['quantity'] < min_purchase].index, inplace=True) \n",
    "    items = df_2['description'].tolist()\n",
    "    items.sort()\n",
    "\n",
    "    # creat new df that contains the filtered out items only\n",
    "    df_filtered = df[df['description'].isin(items)]\n",
    "    df_filtered.sort_values(by='date') \n",
    "\n",
    "    #create new_df_filtered such that dates are equaly spaced out (this is neccessary for Azure ML to run)\n",
    "\n",
    "    #disable SettingWithCopyWarning\n",
    "    pd.options.mode.chained_assignment = None\n",
    "\n",
    "    #iterate through descriptions and map to new datetime slots\n",
    "    days = pd.date_range(start='2016-11-10', end=end_date, periods=time_period) \n",
    "    concat_df = pd.DataFrame(columns = ['date','quantity','description'])\n",
    "\n",
    "    for a in items:\n",
    "        #set up new blank dataframe filled with 0 \n",
    "        new_df_filtered = pd.DataFrame(columns = ['date','quantity','description'])\n",
    "        new_df_filtered.loc[:,'date'] = days\n",
    "        new_df_filtered.fillna(value = 0, inplace = True)\n",
    "        new_df_filtered['description'] = [a]*time_period  \n",
    "\n",
    "        #use a dummy_df to store df with the matching description \n",
    "        dummy_df = df.loc[df['description'] == a]\n",
    "        rows = dummy_df.shape[0]\n",
    "        j = 0 \n",
    "        for i in dummy_df.index.tolist():\n",
    "            while parser.parse(dummy_df['date'][i]) > new_df_filtered['date'][j]:\n",
    "                j = j+1\n",
    "            new_df_filtered['quantity'][j] = new_df_filtered['quantity'][j] + dummy_df['quantity'][i]\n",
    "\n",
    "        #append the dataframe of the new item to the old dataframe\n",
    "        concat_df = pd.concat([concat_df, new_df_filtered], ignore_index=True)\n",
    "    \n",
    "    return concat_df, items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Don't change the code in here'''\n",
    "\n",
    "# define the model training function\n",
    "def train_model(concat_df, items, ws, run_name = 'testrun2-'):\n",
    "\n",
    "    #Load automl packages and define the time column name\n",
    "    from azureml.train.automl import AutoMLConfig\n",
    "    from azureml.train.automl.constants import Metric\n",
    "    time_column_name = 'date'\n",
    "\n",
    "\n",
    "    from azureml.core.experiment import Experiment\n",
    "    from azureml.widgets import RunDetails\n",
    "\n",
    "    #count-for naming purposes \n",
    "    count = 0\n",
    "\n",
    "    #iterate through the dataframe by description \n",
    "    for a in items:\n",
    "        count = count + 1 \n",
    "        df_by_description = concat_df.loc[concat_df['description'] == a]\n",
    "        df_by_description.sort_values(by=['date','description'], ascending = True, inplace = True)  \n",
    "\n",
    "        #Save some data back to make sure we are not overfitting\n",
    "        reserve = 8\n",
    "        X_train = df_by_description[:-reserve]\n",
    "        X_test = df_by_description[-reserve:]\n",
    "\n",
    "        target_column_name = 'quantity'\n",
    "        y_train = X_train.pop(target_column_name).values\n",
    "\n",
    "        time_series_settings = {\n",
    "              \"time_column_name\": time_column_name,\n",
    "    #           \"grain_column_names\": [\"description\"], \n",
    "    #           \"target_lags\": 2,\n",
    "    #           \"target_rolling_window_size\": 5\n",
    "                }\n",
    "\n",
    "        #Local compute \n",
    "        Automl_config = AutoMLConfig(task = 'forecasting',\n",
    "                                     primary_metric = 'normalized_root_mean_squared_error',\n",
    "                                     iteration_timeout_minutes = 3,\n",
    "                                     iterations = 25, # increase this number for more iteration\n",
    "                                     n_cross_validations=5,\n",
    "                                     preprocess = False,\n",
    "                                     enable_stack_ensemble=False, #ensemble tends to overfit\n",
    "                                     enable_voting_ensemble=False,\n",
    "                                     blacklist_models=['DecisionTree'], #decision tree tends to overfit\n",
    "                                     X = X_train,\n",
    "                                     y = y_train,\n",
    "                                     path='',\n",
    "                                     **time_series_settings)\n",
    "\n",
    "\n",
    "        #Run the automl using the local notebook free compute\n",
    "        name = run_name + str(count) + '-' + str(a[:3]) \n",
    "        experiment = Experiment(ws, name)\n",
    "        local_run = experiment.submit(Automl_config, show_output=True)\n",
    "\n",
    "    return(print('training complete'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Change</font> parameter in the code below to execute \n",
    "### <font color='orange'>min_purchase:</font> only create model on items purchased more than 9 times, change this number if wish to model items that are less frequently purchased\n",
    "### <font color='orange'>time_period:</font> change this number to increase/decrease the number of time slots the date will be divided into\n",
    "### <font color='orange'>end_date:</font> date of the latest order recorded\n",
    "\n",
    "##### <font color='red'>time_period = 49 may raise error in the future as end_date changes</font> \n",
    "##### currently no better method other than trail_and_error fo identifying a new time_period what will work\n",
    "##### this problem may be fixed in future version of Azure ML library\n",
    "\n",
    "#### <font color='red'>if you have changed any of those parameters for training your new model, the data shape would have changed, you should then only use the lateset run in your prediction (i.e. use your latest run in notebook 3)</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "df = pd.read_csv('date_description_quantity.csv')\n",
    "\n",
    "# call function to preprocess thed data\n",
    "concat_df, items = preprocess(df, min_purchase = 10, time_period = 49, end_date='2019-05-10') # change parameters here\n",
    "\n",
    "# export concat_df to csv (this csv will be used by notebook '3-predict.ipynb')\n",
    "export_csv = concat_df.to_csv ('export_concat_df.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Change</font> <font color='orange'>run_name</font>  here \n",
    "### The code below trains the models on Azure, so it may take sometime for the training to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed data into model and train models\n",
    "train_model(concat_df, items, ws, run_name = 'testrun4-') # change run_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
